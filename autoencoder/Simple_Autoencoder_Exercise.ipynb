{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Autoencoder\n",
    "\n",
    "We'll start off by building a simple autoencoder to compress the MNIST dataset. With autoencoders, we pass input data through an encoder that makes a compressed representation of the input. Then, this representation is passed through a decoder to reconstruct the input data. Generally the encoder and decoder will be built with neural networks, then trained on example data.\n",
    "\n",
    "<img src='notebook_ims/autoencoder_1.png' />\n",
    "\n",
    "### Compressed Representation\n",
    "\n",
    "A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or oher kinds of reconstruction and transformation!\n",
    "\n",
    "<img src='notebook_ims/denoising.png' width=60%/>\n",
    "\n",
    "In this notebook, we'll be build a simple network architecture for the encoder and decoder. Let's get started by importing our libraries and getting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25926eef630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Autoencoder\n",
    "\n",
    "We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building a simple autoencoder. The encoder and decoder should be made of **one linear layer**. The units that connect the encoder and decoder will be the _compressed representation_.\n",
    "\n",
    "Since the images are normalized between 0 and 1, we need to use a **sigmoid activation on the output layer** to get values that match this input value range.\n",
    "\n",
    "<img src='notebook_ims/simple_autoencoder.png' width=50% />\n",
    "\n",
    "\n",
    "#### TODO: Build the graph for the autoencoder in the cell below. \n",
    "> The input images will be flattened into 784 length vectors. The targets are the same as the inputs. \n",
    "> The encoder and decoder will be made of two linear layers, each.\n",
    "> The depth dimensions should change as follows: 784 inputs > **encoding_dim** > 784 outputs.\n",
    "> All layers will have ReLu activations applied except for the final output layer, which has a sigmoid activation.\n",
    "\n",
    "**The compressed representation should be a vector with dimension `encoding_dim=32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.fc1 = nn.Linear(28*28, encoding_dim)\n",
    "        ## decoder ##\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        x = F.relu(self.fc1(x))\n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Here I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards. \n",
    "\n",
    "We are not concerned with labels in this case, just images, which we can get from the `train_loader`. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing _quantities_ rather than probabilistic values. So, in this case, I'll use `MSELoss`. And compare output images and input images as follows:\n",
    "```\n",
    "loss = criterion(outputs, images)\n",
    "```\n",
    "\n",
    "Otherwise, this is pretty straightfoward training with PyTorch. We flatten our images, pass them into the autoencoder, and record the training loss as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drj\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.654167\n",
      "Epoch: 2 \tTraining Loss: 0.323678\n",
      "Epoch: 3 \tTraining Loss: 0.284317\n",
      "Epoch: 4 \tTraining Loss: 0.274406\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the results\n",
    "\n",
    "Below I've plotted some of the test images along with their reconstructions. For the most part these look pretty good except for some blurriness in some parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images_flatten = images.view(images.size(0), -1)\n",
    "# get sample outputs\n",
    "output = model(images_flatten)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# output is resized into a batch of images\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Up Next\n",
    "\n",
    "We're dealing with images here, so we can (usually) get better performance using convolution layers. So, next we'll build a better autoencoder with convolutional layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
